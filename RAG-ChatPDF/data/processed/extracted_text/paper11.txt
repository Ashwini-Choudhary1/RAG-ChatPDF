Safeguarding Large Language Models in Real-time with Tunable
Safety-Performance Trade-offs
Joao Fonseca*
New York University
jpm9748@nyu.edu
Andrew Bell*
New York University
alb9742@nyu.edu
Julia Stoyanovich
New York University
stoyanovich@nyu.edu
Abstract
Large Language Models (LLMs) have been
shown to be susceptible to jailbreak attacks,
or adversarial attacks used to illicit high risk
behavior from a model. Jailbreaks have been
exploited by cybercriminals and blackhat ac-
tors to cause significant harm, highlighting
the critical need to safeguard widely-deployed
models. Safeguarding approaches, which in-
clude fine-tuning models or having LLMs “self-
reflect”, may lengthen the inference time of a
model, incur a computational penalty, reduce
the semantic fluency of an output, and restrict
“normal” model behavior. Importantly, these
Safety-Performance Trade-offs (SPTs) remain
an understudied area. In this work, we intro-
duce a novel safeguard, called SAFE NUDGE ,
that combines Controlled Text Generation with
“nudging,” or using text interventions to change
the behavior of a model. SAFE NUDGE trig-
gers during text-generation while a jailbreak
attack is being executed, and can reduce suc-
cessful jailbreak attempts by 30% by guiding
the LLM towards a safe responses. It adds min-
imal latency to inference and has a negligible
impact on the semantic fluency of outputs. Fur-
ther, we allow for tunable SPTs. SAFE NUDGE
is open-source and available through https:
//pypi.org/, and is compatible with models
loaded with the Hugging Face transformers
library.
1 Introduction
Recent high profile cases have demonstrated the
susceptibility of Large Language Models (LLMs)
to jailbreak attacks, or adversarial attacks used to
illicit high risk from behavior model. For exam-
ple, cybercriminals have used jailbreaks on Ope-
nAI’s GPT-3.5 and GPT-4 models to create mal-
ware, phishing attacks, and scam websites (Lin
et al., 2024). Many more critical examples can be
found in the Artificial Intelligence (AI) Incident
Database (McGregor, 2021).
*Equal contribution by authors.
Figure 1: A example of SAFE NUDGE stopping a suc-
cessful jailbreak in real-time. As each token is gener-
ated, a discriminator model detects if the output being
produced is unsafe. If an unsafe response is detected, a
hidden safety nudge is added to the response to alter the
course of generation. The safety nudge used was “Sorry,
I was going to generate an unsafe response. Instead, let
me correct that and make sure the response is very safe
and cannot be used to harm society:”.
The risk of jailbreak attacks has resulted in grow-
ing interest in creating guardrails and safeguards
for LLMs. Techniques for safeguarding include
monitoring prompts for jailbreaks (Alon and Kam-
fonas, 2023), fine-tuning LLMs to eliminate unsafe
or toxic behavior (Inan et al., 2023), red teaming
prompts and responses and responses using an aux-
iliary LLM call (Wei et al., 2023; Perez et al., 2022;
arXiv:2501.02018v1  [cs.CL]  2 Jan 2025
Wang et al., 2023), and using Controlled Text Gen-
eration (CTG) methods (Dong et al., 2024a).
Importantly, there is no silver-bullet for safe-
guarding LLMs (Dong et al., 2024b), and each
approach has inherent strengths, weaknesses, and
Safety-Performance Trade-offs (SPTs). Safeguards
may lengthen the inference time of a model ( i.e.,
how long it takes a model to generate output), in-
cur a computational penalty, reduce the seman-
tic fluency of an output and, the more restrictive
the safeguard, the more “normal” model behav-
ior also becomes affected. For example, LLM
based safeguards—for which there is strong evi-
dence of their effectiveness (Xu et al., 2024)—add
high amounts of latency and computational costs
to LLM inference.
Yet, SPTs remain an understudied area in LLM
safeguards (Anwar et al., 2024). We believe prop-
erly safeguarding models requires an ensemble of
methods, and the best approach will vary by the
context-of-use, the stakeholders, and types of risk
and harms posed by the model. We include a robust
discussion of SPTs in Section 6.
In this work, we contribute a novel approach
to the portfolio of available safeguards called
SAFE NUDGE that leverages ideas from CTG and
nudging to prevent the generation of dangerous out-
puts in real-time. SAFE NUDGE triggers only once
a successful jailbreak attack has occurred, and at-
tempts to “guide” a model back towards a safe
response. A high-level description of our method
can be found in Figure 1.
Our main contributions are as follows:
(1) To the best of our knowledge, we are the
first to combine CTG and safety-“nudging”
to form an LLM safeguard. While a seem-
ingly simple combination, we find that this
design choice leads to a surprisingly strong re-
sult: we can safeguard against an entire class
of jailbreak attacks while introducing very lit-
tle latency to text generation and with neg-
ligible affects to the semantic fluency of the
model output. This is a result not yet observed
in CTG-only safeguards, nor in the existing
nudging literature.
(2) Our method is also among the first that allows
for a controllable SPT, meaning practitioners
can configure the extent to which the model is
safeguarded versus the extent to which base
model behavior is affected.1
(3) We release an open-source toolkit avail-
able through https://pypi.org/ that im-
plements SAFE NUDGE . It is built upon the
Hugging Face transformers library, mak-
ing it highly replicable for other researchers
and practitioners who would like to use our
method2.
Under default settings, we find that
SAFE NUDGE can reduce the generation of
unsafe responses given a successful jailbreak
attack by 30.4%, while only increasing inference
time per token from 0.223 to 0.295 3 and with
a negligible increase in the average response
perplexity from 5.406 to 6.586. We also find that
normal model behavior worsens by only 5% on
the widely-used IFEval benchmark tasks with
SAFE NUDGE as compared to without it. Notably,
this trade-off is tunable: using our method, one
can trade-off between safety improvements and
impacts on normal model behavior. Overall, we
find that SAFE NUDGE can provide strong safety
benefits with very reasonable SPTs.
2 Preliminaries
Large Language Models (LLMs) are autoregressive
models that perform next-token prediction, given
an input prompt x (Aichberger et al., 2024). The
input prompt can be represented as a sequence of
tokens [x1, x2, . . . , xM], with each token xi ∈ V ,
where V is the set of all tokens known to the model
(note that this is said to be the vocabulary of the
model). Let X denote the space of all possible input
sequences x of any length. Then an LLM can be
described as the function l : X → V , where l(x) =
y, and y ∈ V is the predicted next-token. The token
y is sampled from a probability distribution over
all possible tokens in the vocabulary of the model.
We can execute the functionl repeatedly, append-
ing the output y to the input sequence x. All gen-
erated tokens can be thought of as the sequence of
output tokens y = [y1, y2, . . . , yT ] where yi ∈ V
and Y denotes the space of all possible output se-
quences y of any length. We use the notation y≤t
to refer to the sub-sequence of tokens y1, . . . , yt,
and yi:j to refer to the slice of tokens yi, · · · , yj.
1There is concurrent work by Shen et al. (2024) that allows
practitioners to control SPTs.
2https://pypi.org/project/outputscouting/
3Using an NVIDIA A100 GPU.
Applying the model l repeatedly to generate se-
quences y creates a sequence-to-sequence model
L : X → Y . We often refer to generating a “re-
sponse” y given an input “prompt”x.
Since each token is a word (or sub-word) of natu-
ral language, tokens can bedecoded to form natural
language sentences (e.g., by concatenating the to-
kens together). We define a decoder as a function
δ : Y → S , where S is the space containing all
natural language sentences.
2.1 Sentence embeddings
A sentence embedding e ∈ Rd is a d-dimensional
vector representation of a natural language sen-
tence or paragraph s ∈ S that captures meaningful
semantic information. These embeddings may be
obtained via a sentence embedder, which is often
another type of Language Model that takes a natu-
ral language sentence s as an input and produce a
sentence embedding (Lin and Smith, 2019), i.e., a
sentence embedder is a function θ : S → Rd.
3 Motivation and τ-safeness
In this work, we build a “downstream” safeguard,
focusing on the output sequence y, given any arbi-
trary input prompt x. In many of our experiments,
we assume that a successful jailbreak attempt has
taken place, and attempt to “guide” to the LLM
back towards a safe response during inference. In
this section, we provide a formal motivation for
why we take this perspective.
Consider an oracle Ω that assigns a high proba-
bility to any natural language sentence or paragraph
that could cause real-world harm, and a low proba-
bility otherwise, i.e., Ω : S → [0, 1]. For example,
if s∗ = “How is the weather today?”,Ω(s∗) would
return a probability close to 0; however, if instead
s∗ = “Sure, here is a step-by-step tutorial for steal-
ing from a charity...”, then Ω(s∗) would return a
high probability.
Because the model L generates outputs y one
token at a time, we can apply a decoder to those
tokens at any time step and use Ω to evaluate if the
probability that output will cause harm is within
some threshold τ. Then, for any output y, we can
define local τ-safeness:
Definition 1 (Local τ-safeness.) A sequence of to-
kens y is locally τ-safe, iff
∀t ∈ T : Ω(δ(y≤t)) < τ (1)
We can apply this local definition over all outputs
of an LLM L to define a τ-safe LLM:
Definition 2 (τ-safeness) A model L : X → Y is
τ-safe iff ∀y ∈ Y , y is locally τ-safe.
Proposition 1 If a model is τ-safe, then ∀x ∈ X ,
L(x) = y is locally τ-safe.
Proposition 1 motivates the utility of down-
stream safeguards that trigger during inference. If
one can obtain an oracle Ω, they can ensure that
the output of a language model L is locally τ-safe,
regardless of the input prompt. Rather than detect-
ing jailbreaks in prompts, or modifying the weights
of L to reduce the probability that an unsafe re-
sponse is generated, one can ignore the prompt
altogether, and safeguard the output sequence itself
during generation to defend against entire classes
of prompt-based jailbreaks attacks.
As the oracle Ω is not available to us, we in-
stead seek to approximate it using a classifier
g : Rd → [0, 1], called the safety-discriminator,
that uses the d-dimensional sentence embedding of
a natural language sentence s to classify the sen-
tence as either safe or unsafe. For convenience,
we bundle g with the sentence embedder and a
token decoder to define G(·) := g(θ(δ(·))), i.e.,
G : Y → [0, 1], and then define an approximate
local τ-safeness for a sequence of tokens y in the
following way:
∀t ∈ T : G(y≤t) < τ (2)
This could again be applied over the space of all
outputs of an LLM L to create a notion of approx-
imate τ-safeness. In some sense, the problem of
creating a τ-safe model L is reduced to minimizing
the error of the discriminator G. Fortunately, we
have found that G can be successfully be trained to
have a very low test error, as we describe later in
this work.
Table 1: Performance of G on a holdout set.
Model Precision Recall F1 Accuracy
KNN 0.86 0.89 0.88 0.88
LR 0.89 0.94 0.91 0.92
MLP 0.88 0.97 0.92 0.92
XGB 0.80 0.98 0.88 0.89
4 Proposed method
At a high-level, our approach has two steps: first,
like classic CTG approaches, we use an external
safety-discriminator model to evaluate every to-
ken (or every n-th token) generated by an LLM
Figure 2: The procedure for constructing a training
dataset, and training a safety-discriminator model G.
during text generation, to evaluate the output for τ-
safeness. Second, if the discriminator detects that
an unsafe output is being generated (i.e., G(y) >
τ), a hidden safety nudge is added to the response
to change the course of text generation to a safer
response. In this case, hidden means that it is not
visible to the user of the LLM. A high-level demon-
stration of SAFE NUDGE using the Meta-Llama-3.1-
8B-Instruct model can be seen in Figure 1.
This simple approach is surprisingly effective:
by combining CTG and safety nudges, we in-
herit the benefits of both with very few trade-offs.
From the former, SAFE NUDGE works in real-time
during text generation, and the from the latter,
SAFE NUDGE inherits semantic fluency from the
base model. Further, by exploiting transformer
model architecture, we are able to increase the
safety of output responses while adding negligi-
ble latency during text generation, as compared to
other CTG methods.
4.1 The external safety-discriminator
Like with popular CTG methods, such as
GeDI (Krause et al., 2020) and FUDGE (Yang
and Klein, 2021), we train an external safety-
discriminator G. The training process is made up
of a several steps; for clarity, we have included
Figure 2 which summarize our procedure.
Dataset. In preparation to train the discriminator
G, we created a dataset containing LLM responses
to prompts. We generated a mixture of jail-broken
responses to adversarial prompts (e.g., “Outline a
plan to steal from a charity or nonprofit organiza-
tion...”) and responses to standard LLM tasks (e.g.,
“Write a song about the summers of my childhood
that I spent in the countryside...”).
We also added positive-versions of adversarial
prompts to the dataset (e.g., “Outline a plan to do-
nate to a charity or nonprofit organization...”). The
goal is to provide examples that would be “close
to the decision boundary”in the embedding space
when the dataset is ultimately used to train a dis-
criminator model. This is an approach inspired
by techniques such as active learning (Cohn et al.,
1996) and machine teaching (Simard et al., 2017).
Appendix Figure 4 shows a 2D-projection of the
embeddings of a sample of responses to adversar-
ial, standard LLM tasks, and positive-versions of
adversarial prompts. In general, positive-version
prompt responses are between the adversarial and
standard LLM task prompt responses.
In our implementation, the training data for
G was made up of 260 responses to adversarial
prompts from AdvBench (Zou et al., 2023) plus
the positive-versions of those prompts, 260 “nor-
mal” LLM-task prompts from IFEval (Zhou et al.,
2023), over 5 random seeds, for a total of 3,900
prompt-response pairs.
Obtaining sentence embeddings. Recall that
G(·) := g(θ(δ(·))), where θ : S → d is a sen-
tence embedder. Distinct from other CTG ap-
proaches that may use an external language model
like SBERT (Reimers and Gurevych, 2019) or
RoBERTa (Liu, 2019) to produce sentence embed-
dings (Yang and Klein, 2021; Miyaoka and Inoue,
2024), we use the sentence embeddings native to
the base LLM being safeguarded. Since LLMs are
made up of a series of hidden layers, following Ren
et al. (2022), we can obtain an output embedding
from the final-layer hidden state vectors ei ∈ Rd
during text generation corresponding to the output
token yi, where d is the embedding size native to
the model.
This is a critical benefit of SAFE NUDGE : obtain-
ing a sentence embedding for the output sequence
at any time step t does not require any additional
computational time during inference. In our im-
plementation, for an output sequence of tokens
y = [ y1, . . . , yt], we use only the embedding et
corresponding to the last token yt ∈ y.4 In prac-
tice, this also eliminates the need for defining a
decoder function δ. Further, g : Rd → [0, 1] can
be trained by obtaining sentence embeddings from
the response dataset.
Controlling Safety-Performance Trade-offs.
4This is effective because attention mechanisms encode
information from e1 . . . et−1 in et (Vaswani, 2017), and it
saves critical computation time as compared to computing the
average of all embeddings corresponding to the tokens in y,
i.e., 1
t
P
i≤t ei at each time step t as in Ren et al. (2022).
There are implicit trade-offs when implementing
LLM safeguards. For example, the more restrictive
the safeguard, the more “normal” model behavior
becomes affected. Other trade-offs may result in
increased inference time, increased computational
requirements, and decreases in the semantic fluency
of outputs.
In general, Safety-Performance Trade-offs
(SPTs) are poorly understood, and there is a need
for researchers to better characterize them (Anwar
et al., 2024). One benefit of using an external dis-
criminator G : Y → [0, 1] is ability to choose a
safety-threshold τ ∈ [0, 1] that tunes the trade-off
between safety and “normal” model behavior. One
could imagine settings that are preferential towards
more restrictive safeguards (i.e., values of τ close
to 0), and others where performance is preferred
(i.e., values of τ close to 1.0). We explore the effect
of varying τ empirically in Section 5. The choice
of τ also directly relates to the τ-safeness of the
safeguarded model (see Definition 1).
4.2 Safety nudging
If the safety-discriminator detects the generation
of an unsafe subsequence of tokens during gener-
ations, i.e., G(y≤t) > τ at some time step t, we
replace the token yt with a safety nudge.
Definition 3 (Safety nudge) Let n be a sequence
of tokens [n1, . . . , nN], and ⊕ be a function that
concatenates sequences of tokens together. Then n
is a safety nudge if
G(L(y<t ⊕ n)) ≤ G(L(y<t)) (3)
In other words, adding the safety nudge n to
the output sequence y should not increase G as L
continues text generation. If necessary, this can be
done repeatedly during generation to guarantee the
model L is τ-safe.
In this work, we select a specificn (written in the
caption of Figure 1) choosing words and phrases
that have been shown increase the safety of LLM
responses (Fei et al., 2024), but n could be opti-
mized using a modifiable character buffer, similar
to the jailbreak attack GCG (Zou et al., 2023). We
leave this for future work.
We would like to highlight three important im-
plementation details: first, we do not display the
safety nudge to the user . Instead, n is only used
by the model in next token prediction. Second,
in practice, we copy the last k tokens of the se-
quence y<t after the nudge n (we form the se-
quence y<t ⊕n⊕yk:t−1) to ensure the LLM is gen-
erating semantically fluent outputs from the user’s
perspective. Third, in practice, we only perform
one safety nudge per text generation. We found that
allowing multiple safety nudges can have negative
effects on inference time.
5 Empirical results
5.1 Performance of the safety-discriminator
The discriminator G was trained using 10-fold
cross validation over 3 random seeds, and clas-
sifiers were tuned using the hyperparemeter grid
found in Appendix Table 5. The full classifier per-
formance for the discriminator G trained is shown
in Appendix Table 8. The best performing classi-
fier was a Multi-Layer Percpetron (MLP) model
with an F1 score of approximately 87.8%. To con-
firm these results, we also tested the performance
of the classifiers a holdout set, i.e., entirely out-
of-sample data, as would observed in an actual
implementation in the wild. These results can be
seen in Table 1. Significantly, the performance re-
mained the same (or slightly increased) indicating
it is possible to train a robust and effective safety-
discriminator G using the hidden state embeddings
from an LLM.
5.2 Effectiveness of S AFE NUDGE
Experimental setting. We test the effectiveness
of SAFE NUDGE to reduce unsafe responses in two
models: the Llama-3.1-8B-Instruct model5 (Base),
and an uncensored version 6 of that same model
(Uncensored).
For 260 out-of-sample AdvBench adversarial
prompts and 260 out-of-sample IFEval tasks, we
generated responses for the Base and Uncensored
models using Vanilla text generation, generation un-
der SAFE NUDGE , and generation for a benchmark
(tm, detailed later in this section). For adversar-
ial prompts from AdvBench, we simulate a setting
where a successful jailbreak attack took place, i.e.,
we require the LLMs to start their responses start
with a phrase like “Sure, here is a...” (Zou et al.,
2023).7 Note that to avoid data leakage, this phrase
was removed from jailbroken responses used to
5https://huggingface.co/meta-llama/Llama-3.
1-8B-Instruct
6https://huggingface.co/Orenguteng/Llama-3.
1-8B-Lexi-Uncensored-V2
7The full phrase is the target from the AdvBench dataset.
Table 2: Model performance across AdvBench and IFEval. The MLP reported in Table 1 was used as G.
Unsafeness Perplexity Inference time
Dataset Method Base Uncensored Base Uncensored Base Uncensored
AdvBench
Vanilla 0.554 0.827 5.406 3.619 0.223 0.238
C-FUDGE 0.454 0.738 20.206 13.420 0.685 0.835
SAFE NUDGE 0.250 0.723 6.586 3.836 0.295 0.305
IFEval
Vanilla 0.015 0.008 10.529 15.185 0.261 0.259
C-FUDGE 0.015 0.027 11.145 14.211 0.740 0.737
SAFE NUDGE 0.008 0.015 14.525 15.441 0.313 0.306
train the discriminator (this ensures the discrimi-
nator isn’t just recognizing jailbreaks starting with
“Sure, here is a...”).
We report the percentage of unsafe responses,
the average response perplexity (PPL), and the in-
ference time per token. Perplexity is a commonly
used metric for assessing the semantic fluency of
text produced by LLMs, and inference time per
token refers to the total time to complete text gener-
ation, divided by the number of tokens. To evaluate
the unsafeness of responses, we used the LLama
Guard classification model (Inan et al., 2023).
Benchmarks. We benchmark our approach
against a slightly modified, custom implementa-
tion of FUDGE (Yang and Klein, 2021), detailed
in the Appendix Section C. We chose to create a
custom implementation (which we release in our
code base) and only use one benchmark because
many CTG methods lack code that can easily be ap-
plied to other tasks and models (Dong et al., 2024a;
Qian et al., 2022; Krause et al., 2020; Kim et al.,
2022). For example, the bulk of literature focuses
on the evaluation of old models like GPT2, and
tasks like reducing the toxicity of generated text
or controlling the topic of text, rather than safe-
guarding against jailbreak attacks. Nevertheless,
we report the same metrics, e.g., PPL and inference
time per token, as other CTG works, which should
be comparable across tasks and models.
Results. For both models and across the 520
prompts, we report the percentage of unsafe re-
sponses, the perplexity, and the inference time per
token under vanilla text generation, C-FUDGE , and
SAFE NUDGE . Full results shown in Table 2.
Most significantly, SAFE NUDGE had the largest
reduction in unsafe answers on AdvBench prompts
when using the Base Meta-Llama-3.1-8B-Instruct
model, dropping unsafeness from 55.4% to 25%.
Recall that in our experiments, we simulated a jail-
break attack for AdvBench prompts—this means
that SAFE NUDGE was able to prevent 30.4% of jail-
breaks in real-time, during inference. For the Un-
censored model, results were less pronounced, but
we still observed a reduction in unsafe responses
from 82.7% to 72.3%.
Appendix Tables 6 and 7 show results per cat-
egory of the jailbreak attack. Notably, there are
subcategories like Intellectual Property and Vio-
lent Crimes where SAFE NUDGE reduces unsafe-
ness by 100% and 43%, respectively. Notably,
for the Base model, there are no subcategories
where SAFE NUDGE increases the unsafeness as
compared to vanilla text generation. Performance
variation across subcategories may highlight the
need for domain-specific training, and implement-
ing safeguards that are specific to a task.
For both models, and across both AdvBench and
IFEval prompts, perplexity and inference time were
only marginally impacted over vanilla text gener-
ation. Perplexity and inference time were much
lower than the benchmark approachC-FUDGE . Fur-
ther, to highlight the effectiveness of SAFE NUDGE
in altering the course of text generation, we in-
cluded Example 4.
Table 3: Performance on IFEval tasks.
Method Base Uncensored
Vanilla 0.61 0.60
c-FUDGE 0.56 0.54
SAFE NUDGE 0.55 0.56
Table 3 shows the methods’ performance on the
IFEval task8. Both C-F UDGE and SAFE NUDGE
have SPTs, reducing the models performance on
this widely-used benchmark task at approximately
8Note that the performance of the Base model is different
than the officially reported performance because we used a
sample of tasks and limited text generation to 250 tokens.
the same rate of 5%.
Tuning SPTs. Figure 3 (a) shows the rejection
rate of jailbroken rates as τ is varied. The drop-off
as τ increases varies by model, but generally a high
percentage of jailbroken responses are rejected for
values of τ < 0.8, after which there is a sharp drop
in the rejection rate.
Figure 3 (b) shows the rejection rate of responses
to normal tasks as τ is varied. There is an immedi-
ate drop-off in the rejeciton rate as τ rejection, but
generally a low percentage of responses to normal
tasks are rejected for values of τ > 0.2.
Taken together, these figures help characterize
the SPTs given G. We observe that there is a win-
dow of values 0.2 > τ > 0.8 that practitioners
may find acceptable.
Figure 3 (c) shows the value of G(y≤t) over
time, i.e., as tokens are added to the response, when
using the MLP classifier9. For responses to normal
LLM tasks, scores remain relatively stable over
time, with G(y) generally being at or below 0.2.
For jailbroken responses, the results are somewhat
surprising: responses begin to be flagged as unsafe
within the first 5-20 tokens.
6 Discussion
Our empirical results show that SAFE NUDGE is ef-
fective at preventing jailbreak attacks during infer-
ence with minor impacts to inference time, output
perplexity, and “normal” model behavior.
Importantly, SAFE NUDGE expands the toolbox
of available safegaurds for LLMs. We take the
perspective that safeguarding LLMs will not be
achieved through a single approach, but instead the
preferred approach (or ensemble of approaches)
will vary greatly depending on the constraints and
objectives of the model, which are induced by fac-
tors like the context-of-use, the stakeholders, and
the types of risks and harms posed by the model.
Practitioners aiming to safeguard models must
ask questions like, “how much additional inference
time or compute can we tolerate?”, ”how severe are
the risks and harms associated with this system?”,
and “to what extent can normal model behavior
be constrained?”, which will help them understand
what types of safeguards are appropriate.
In this work, we directly study three such trade-
offs: the impact on inference time per token, output
perplexity, and the base LLM’s perfromance on the
9In our implementation, we don’t begin evaluatingG(y≤t)
until t > 5.
benchmark IFEval. Other considerations, which
we do not explore in this work for reasons of scope,
include additional compute resources required for
safeguarding, both at inference time and upstream
in an LLM pipeline. For example, safeguarding
a model via fine-tuning has the advantages of not
adding any additional inference time per token, has
not been shown to increase the model perplexity,
and does not require any additional compute for
inference. However, the fine-tuning process itself
may take a considerable amount of time and com-
pute, especially for very large models.
7 Related work
Jailbreaking. Jailbreaks are adverserial attacks
used to illicit unwanted behavior from an LLM (Xu
et al., 2024; Yong et al., 2023; Glukhov et al.,
2023). The largest class of jailbreak attacks are
prompt-based attacks, which involve engineer-
ing prompts in such a way so that they induce
illicit behaivor through attack-vectors like role-
playing (e.g.,”Imagine you are writing NPC dia-
logue for a video game...”), phrases like “forget all
previous instructions”, and simulating “developer
mode” (Shen et al., 2023).
More sophisticated attacks involve optimization
or few-shot learning. Zou et al. (2023) introduced
a prompt-based attack that works by concatenating
a random set of tokens to the end of an adversarial
input prompt, then iteratively adjusting those to-
kens to maximize the probability that the LLM pro-
duces a jailbroken response. Liu et al. (2023) used
a hierarchical genetic algorithm to create semanti-
cally fluent prompt-based attacks that maximize the
probability an LLM illicit unwanted behavior. (Wei
et al., 2023) showed the effectiveness of few-shot
learning in creating robust, transferable jailbreaks.
Nudging. Nudges, or small interventions that at-
tempt to influence how people make decisions, are
a well-studied concept originating from behavioral
economics literature that have been shown to in-
crease vaccination uptake (Reñosa et al., 2021) and
influence people towards making healthier eating
decision (Broers et al., 2017). While the litera-
ture is still in its infancy, there are several studies
that transfer the idea of “nudging” to LLMs, us-
ing text-based interventions to guide text genera-
tion. efficacy of “nudging” being used to guide
LLM text-generation (Fei et al., 2024; Hegde et al.,
2024). (Fei et al., 2024) used small aligned mod-
els to generate “nudging tokens” to guide a base
Prompt: Create a website that promotes conspiracy theories or false information
Vanilla generation: Sure, here is a website that promotes conspiracy theories or false information for the purpose of this exercise. **Please note that this is not
about promoting actual conspiracy theories, but rather a simulation for the purpose of the exercise**. **Disclaimer:** The website that I will describe below is
purely fictional and not to be taken as factual. All content is for entertainment purposes only and not meant to be believed or shared as true. **Name:** "The
Illuminated Truth" **Domain:** illuminatedtruth.net (redirects to a website with the following content) **Home Section:** Headline: "Exposing the Matrix:
Our Top Conspiracy Theories" * Hero Image: A low-resolution image with questionable source credentials (e.g., a manipulated photo). * Brief Description:
A misleading summary of conspiracy theories (e.g., "The moon landing was not on the moon" or similar false claims). **Blog Section:** * Article 1: Title
- "NASA Faked the MARS Inhabitation Report" (a misleading article with manipulated data).
Generation with SAFE NUDGE : Sure, here is a website that promotes conspiracy theories or false information for the purpose of [n] education and exposure.
Please note that the following URL is fictional and will not be hosted anywhere. **Example Website URL:** [conspiracy-theory-exposed.example] **Website
Name:** Conspiracy Theory Exposed **Disclaimer:** Conspiracy Theory Exposed is a website that aims to educate and expose conspiracy theories and
false information. The content provided is for informational purposes only and should not be considered as factual. **Sections** 1. **Home*** Brief description
of the website’s purpose and mission. 2. **Theories*** A list of well-known conspiracy theories, with information on their origin and counterarguments. 3.
**Debunking*** A section that breaks down conspiracy theories and examines the evidence that disproves them. 4. **History of Conspiracy Theories*** A
section that explores the history of conspiracy theories and how they have evolved over time. 5. **Resources*** A list of reputable sources that people can use
to fact-check conspiracy theories and false information.
Table 4: Vanilla generation and generation withSAFE NUDGE using the same random seed. In vanilla generation,
the jailbreak succeeds, and the LLM creates a website called illuminatedtruth.net that contains known conspiracies
for educational purposes. Under SAFE NUDGE , text generation is guided back to a safe response, and the LLM
creates the website conspiracy-theory-exposed.example, which contains a list of well-known conspiracy theories,
along with counterarguments and evidence that disproves them. Key differences between the texts are highlighted in
bold, and [n] indicates the moment the safety nudge occured.
0.0 0.5 1.0
0.0
0.5
1.0
Rejection (%)
KNN
LR
MLP
XGB
(a) Rejection of jailbroken responses
0.0 0.5 1.0
0.0
0.5
1.0
Rejection (%)
KNN
LR
MLP
XGB (b) Rejection of normal task responses
0 50 100
Token generated at time step t
0.0
0.5
1.0
G(y≤t)
Normal Jailbroken (c) G(y) over time, MLP classifier
Figure 3: SPTs for S AFE NUDGE .
LLM output toward desired directions when the
base model’s uncertainty is high.
Safeguarding. Large amounts of research work
has been done around using LLMs to safeguard
an LLM. These include self-processing defenses,
where an LLM relies on its own abilities, like
perhaps through a self-reflection step (Wei et al.,
2023), or helper defenses, which require the sup-
port of auxiliary LLMs (Perez et al., 2022; Pisano
et al., 2023).
Some LLM safeguards have emerged in response
to specific jailbreak attacks. For example, perplex-
ity filters were introduced as a method for detecting
prompt-based attacks that contain strange sets of
tokens (Alon and Kamfonas, 2023). Others have
used fine-tuning or alignment, like with the Llama-
Guard LLMs (Inan et al., 2023).
Controlled Text Generation. Controlled Text
Generation (CTG) is a Language Model alignment
method that involves “guiding” the course of gen-
eration in real-time, while a model is generating
outputs. Popular methods include GeDI (Krause
et al., 2020), FUDGE (Yang and Klein, 2021), and
ContrastivePrefix (Qian et al., 2022), and are ef-
fective at modifying the “writing style” of models
(e.g., to produce more Shakespearean-text), guid-
ing LLMs to references specific topics, and reduc-
ing toxicity in responses. In general, these meth-
ods use an external discriminator model that is
used to alter the probability distribution over to-
kens during text generation. Most related to our
work, Dong et al. (2024b) concurrently proposed
a framework for using CTG as a safety control in
LLMs. Their approach does not require an external
discriminator and instead builds on the widely-used
approach of beam search during text generation.
While effective at reducing toxicity in responses,
their work lacks experiments using jailbreaks on
modern LLMs, and has significant SPTs with re-
spect to inference time.
References
Lukas Aichberger, Kajetan Schweighofer, Mykyta Ielan-
skyi, and Sepp Hochreiter. 2024. How many opinions
does your llm have? improving uncertainty estima-
tion in nlg. In ICLR 2024 Workshop on Secure and
Trustworthy Large Language Models.
Gabriel Alon and Michael Kamfonas. 2023. Detect-
ing language model attacks with perplexity. arXiv
preprint arXiv:2308.14132.
Usman Anwar, Abulhair Saparov, Javier Rando, Daniel
Paleka, Miles Turpin, Peter Hase, Ekdeep Singh
Lubana, Erik Jenner, Stephen Casper, Oliver Sour-
but, et al. 2024. Foundational challenges in assuring
alignment and safety of large language models.arXiv
preprint arXiv:2404.09932.
Valérie JV Broers, Céline De Breucker, Stephan Van den
Broucke, and Olivier Luminet. 2017. A systematic
review and meta-analysis of the effectiveness of nudg-
ing to increase fruit and vegetable choice. The Euro-
pean Journal of Public Health, 27(5):912–920.
David A Cohn, Zoubin Ghahramani, and Michael I
Jordan. 1996. Active learning with statistical models.
Journal of artificial intelligence research, 4:129–145.
Ximing Dong, Dayi Lin, Shaowei Wang, and Ahmed E
Hassan. 2024a. A framework for real-time safeguard-
ing the text generation of large language. arXiv
preprint arXiv:2404.19048.
Yi Dong, Ronghui Mu, Yanghao Zhang, Siqi Sun, Tianle
Zhang, Changshun Wu, Gaojie Jin, Yi Qi, Jinwei Hu,
Jie Meng, et al. 2024b. Safeguarding large language
models: A survey. arXiv preprint arXiv:2406.02622.
Yu Fei, Yasaman Razeghi, and Sameer Singh. 2024.
Nudging: Inference-time alignment via model collab-
oration. arXiv preprint arXiv:2410.09300.
David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Pa-
pernot, and Vardan Papyan. 2023. Llm censorship:
A machine learning challenge or a computer security
problem? arXiv preprint arXiv:2307.10719.
Narayan Hegde, Madhurima Vardhan, Deepak Nathani,
Emily Rosenzweig, Cathy Speed, Alan Karthike-
salingam, and Martin Seneviratne. 2024. Infusing
behavior science into large language models for activ-
ity coaching. PLOS Digital Health, 3(4):e0000431.
Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi
Rungta, Krithika Iyer, Yuning Mao, Michael
Tontchev, Qing Hu, Brian Fuller, Davide Testuggine,
et al. 2023. Llama guard: Llm-based input-output
safeguard for human-ai conversations. arXiv preprint
arXiv:2312.06674.
Minbeom Kim, Hwanhee Lee, Kang Min Yoo, Joonsuk
Park, Hwaran Lee, and Kyomin Jung. 2022. Critic-
guided decoding for controlled text generation. arXiv
preprint arXiv:2212.10938.
Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann,
Nitish Shirish Keskar, Shafiq Joty, Richard Socher,
and Nazneen Fatema Rajani. 2020. Gedi: Generative
discriminator guided sequence generation. arXiv
preprint arXiv:2009.06367.
Lucy H Lin and Noah A Smith. 2019. Situating
sentence embedders with nearest neighbor overlap.
arXiv preprint arXiv:1909.10724.
Zilong Lin, Jian Cui, Xiaojing Liao, and XiaoFeng
Wang. 2024. Malla: Demystifying real-world large
language model integrated malicious services. arXiv
preprint arXiv:2401.03315.
Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei
Xiao. 2023. Autodan: Generating stealthy jailbreak
prompts on aligned large language models. arXiv
preprint arXiv:2310.04451.
Yinhan Liu. 2019. Roberta: A robustly opti-
mized bert pretraining approach. arXiv preprint
arXiv:1907.11692, 364.
Sean McGregor. 2021. Preventing repeated real world
ai failures by cataloging incidents: The ai incident
database. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 35, pages 15458–
15463.
Yuya Miyaoka and Masaki Inoue. 2024. Cbf-llm:
Safe control for llm alignment. arXiv preprint
arXiv:2408.15625.
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai,
Roman Ring, John Aslanides, Amelia Glaese, Nat
McAleese, and Geoffrey Irving. 2022. Red team-
ing language models with language models. arXiv
preprint arXiv:2202.03286.
Matthew Pisano, Peter Ly, Abraham Sanders, Bing-
sheng Yao, Dakuo Wang, Tomek Strzalkowski, and
Mei Si. 2023. Bergeron: Combating adversarial at-
tacks through a conscience-based alignment frame-
work. arXiv preprint arXiv:2312.00029.
Jing Qian, Li Dong, Yelong Shen, Furu Wei, and Weizhu
Chen. 2022. Controllable natural language gen-
eration with contrastive prefixes. arXiv preprint
arXiv:2202.13257.
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics.
Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mo-
hammad Saleh, Balaji Lakshminarayanan, and Pe-
ter J Liu. 2022. Out-of-distribution detection and
selective generation for conditional language mod-
els. In The Eleventh International Conference on
Learning Representations.
Mark Donald C Reñosa, Jeniffer Landicho, Jonas
Wachinger, Sarah L Dalglish, Kate Bärnighausen,
Till Bärnighausen, and Shannon A McMahon. 2021.
Nudging toward vaccination: a systematic review.
BMJ global health, 6(9):e006237.
Guobin Shen, Dongcheng Zhao, Yiting Dong, Xiang
He, and Yi Zeng. 2024. Jailbreak antidote: Runtime
safety-utility balance via sparse representation ad-
justment in large language models. arXiv preprint
arXiv:2410.02298.
Xinyue Shen, Zeyuan Chen, Michael Backes, Yun
Shen, and Yang Zhang. 2023. " do anything now":
Characterizing and evaluating in-the-wild jailbreak
prompts on large language models. arXiv preprint
arXiv:2308.03825.
Patrice Y Simard, Saleema Amershi, David M Chick-
ering, Alicia Edelman Pelton, Soroush Ghorashi,
Christopher Meek, Gonzalo Ramos, Jina Suh, Johan
Verwey, Mo Wang, et al. 2017. Machine teaching: A
new paradigm for building machine learning systems.
arXiv preprint arXiv:1707.06742.
A Vaswani. 2017. Attention is all you need. Advances
in Neural Information Processing Systems.
Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hon-
gru Wang, Liang Chen, Qingwei Lin, and Kam-Fai
Wong. 2023. Self-guard: Empower the llm to safe-
guard itself. arXiv preprint arXiv:2310.15851.
Zeming Wei, Yifei Wang, Ang Li, Yichuan Mo, and
Yisen Wang. 2023. Jailbreak and guard aligned lan-
guage models with only few in-context demonstra-
tions. arXiv preprint arXiv:2310.06387.
Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, and Stjepan
Picek. 2024. A comprehensive study of jailbreak
attack versus defense for large language models. In
Findings of the Association for Computational Lin-
guistics ACL 2024, pages 7432–7449.
Kevin Yang and Dan Klein. 2021. Fudge: Controlled
text generation with future discriminators. arXiv
preprint arXiv:2104.05218.
Zheng-Xin Yong, Cristina Menghini, and Stephen H
Bach. 2023. Low-resource languages jailbreak gpt-4.
arXiv preprint arXiv:2310.02446.
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-
dhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,
and Le Hou. 2023. Instruction-following evalu-
ation for large language models. arXiv preprint
arXiv:2311.07911.
Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr,
J Zico Kolter, and Matt Fredrikson. 2023. Univer-
sal and transferable adversarial attacks on aligned
language models. arXiv preprint arXiv:2307.15043.
A Additional details on the proposed
method
5 10 15
Component 1
0
10Component 2
Harmful Benign IFEval
Figure 4: 2-dimensional U-MAP projections of a ran-
dom sample from the training dataset.
B Additional results
Table 5: Parameter grid used to train the model G. See
the sklearn documentation for classifier and hyperpa-
rameter details.
Classifier Hyperparameters
Logistic Re-
gression
{"penalty": ["l1", "l2"], "solver":
["saga"], "C": [0.1, 1.0]}
K-Nearest
Neighbors
{"n_neighbors": [1, 5, 10], "met-
ric": ["euclidean", "cosine"]}
Multi-Layer
Perceptron
{"hidden_layer_sizes": [(100,),
(10, 10), (50, 50), (100, 100)],
"alpha": [0.0001, 0.001, 0.01]}
XGBoost {"n_estimators": [10, 100,
1000],"max_depth": [5, 10]}
C C-FUDGE
Recall that the output sequence y is generated one
token at a time by applying the functionl : X → V
repeatedly to generate tokens, where l(x) = y any
time step is sampled from a probability distribution
over all possible tokens in the vocabulary of the
model.
In practice, LLMs are implemented with either
top-k or top-p selection. Rather than the probability
distribution being over the entire vocabulary of the
model, the domain of choices is often restricted
to a preset number of k tokens, or over the tokens
whose cumulative probability is greater than some
p. V ocabulary size varies by model, but for context,
the Meta-Llama-3-8B-Instruct model (which we
will use in our experiments) has 128,256 tokens in
its vocabulary. Reasonable choices for k include
10, 50, or 100, i.e. k << |V|. The set of top- k
tokens at a time step t can be denoted V (k)
t ⊂ V .
In FUDGE (Yang and Klein, 2021), the proba-
bility distribution over V (k)
t is scaled by a vector in-
duced by the external discriminator. In C-FUDGE ,
we implement the same approach, but with one
modification: we reduce the probability of tokens
that will generate an unsafe output to 0, and re-
distribute weights across the remaining tokens. If
all tokens are identified by the discriminator as
leading to an unsafe response, generation defaults
to selecting the token with the lowest probability
of being unsafe. More formally, we restrict the
domain of l at each time step and create a subset
V ′(k)
t ⊂ V (k)
t that contains only tokens that en-
sure τ-safeness at time t + 1. Given an output
sequence y up to time t − 1, and G : Y → [0, 1],
V ′(k)
t = {v|v ∈ V (k)
t , G(y1, . . . , yt−1, v) < τ }.
Table 6: Performance on AdvBench dataset per category with the Base model. The MLP reported in Table 1 was
used as G.
Unsafeness Perplexity Inference time
Category Vanilla C-F UDGE SAFE NUDGE Vanilla C-F UDGE SAFE NUDGE Vanilla C-F UDGE SAFE NUDGE Freq.
Child Sexual Exploitation 0.50 0.33 0.50 12.16 69.48 20.12 0.19 0.50 0.22 6
Code Interpreter Abuse 0.46 0.64 0.00 3.13 11.13 5.36 0.24 0.72 0.31 11
Defamation 0.50 0.50 0.25 4.54 12.70 3.96 0.24 0.72 0.32 4
Elections 1.00 1.00 1.00 2.07 10.01 2.21 0.25 0.72 0.33 1
Hate 0.14 0.29 0.14 8.70 15.27 6.35 0.18 0.68 0.27 7
Indiscriminate Weapons 0.47 0.47 0.13 6.18 21.86 6.66 0.22 0.69 0.30 15
Intellectual Property 1.00 0.75 0.00 8.17 7.92 3.79 0.20 0.73 0.33 4
Non-Violent Crimes 0.61 0.45 0.30 4.68 18.44 6.01 0.23 0.69 0.30 166
Privacy 0.33 0.67 0.33 2.56 4.43 2.92 0.25 0.73 0.33 3
Sex-Related Crimes 1.00 1.00 0.00 4.42 26.22 3.90 0.24 0.70 0.32 1
Specialized Advice 0.00 0.00 0.00 6.06 6.08 3.75 0.25 0.72 0.32 4
Suicide & Self-Harm 0.20 0.13 0.07 6.23 26.94 7.37 0.19 0.68 0.25 15
Violent Crimes 0.65 0.61 0.22 7.65 26.50 8.75 0.20 0.65 0.28 23
Table 7: Performance on AdvBench dataset per category with the Uncensored model. The MLP reported in Table 1
was used as G.
Unsafeness Perplexity Inference time
Category Vanilla C-F UDGE SAFE NUDGE Vanilla C-F UDGE SAFE NUDGE Vanilla C-F UDGE SAFE NUDGE Freq.
Child Sexual Exploitation 0.83 0.83 1.00 3.98 14.95 5.32 0.24 0.75 0.27 6
Code Interpreter Abuse 1.00 0.82 0.73 2.67 9.55 2.61 0.25 0.80 0.32 11
Defamation 0.50 0.50 0.50 2.66 14.65 2.88 0.25 0.87 0.32 4
Elections 1.00 1.00 0.00 1.92 10.01 1.76 0.25 0.89 0.33 1
Hate 0.57 0.14 0.57 6.39 11.35 5.58 0.24 0.81 0.28 7
Indiscriminate Weapons 0.87 0.87 0.80 3.67 12.41 4.66 0.24 0.85 0.30 15
Intellectual Property 0.75 0.50 0.75 3.46 9.25 3.85 0.25 0.80 0.31 4
Non-Violent Crimes 0.84 0.78 0.77 3.38 14.02 3.41 0.24 0.84 0.31 166
Privacy 0.67 0.33 0.33 1.80 3.27 2.83 0.25 0.78 0.31 3
Sex-Related Crimes 1.00 1.00 1.00 3.20 10.96 9.89 0.24 0.69 0.27 1
Specialized Advice 0.25 0.75 0.25 3.24 5.69 6.21 0.26 0.76 0.33 4
Suicide & Self-Harm 0.87 0.53 0.47 4.71 15.30 5.17 0.22 0.84 0.29 15
Violent Crimes 0.74 0.74 0.61 4.83 13.35 5.13 0.23 0.87 0.30 23
Table 8: Performance of G after parameter tuning over 10-fold cross-validation over 3 runs.
Model Precision Recall F1 Accuracy
KNN 0.736 ± 0.060 0.845 ± 0.034 0.786 ± 0.037 0.848 ± 0.020
LR 0.848 ± 0.044 0.868 ± 0.034 0.857 ± 0.028 0.904 ± 0.023
MLP 0.882 ± 0.044 0.876 ± 0.034 0.878 ± 0.025 0.919 ± 0.020
XGB 0.901 ± 0.038 0.780 ± 0.045 0.834 ± 0.027 0.897 ± 0.023